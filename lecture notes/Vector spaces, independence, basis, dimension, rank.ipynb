{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vector spaces, independence, basis, dimension, rank**\n",
    "\n",
    "---\n",
    "\n",
    "### **Author**\n",
    "**Junichi Koganemaru**  \n",
    "\n",
    "---\n",
    "\n",
    "### **References**\n",
    "1. Gilbert Strang, Introduction to Linear Algebra.\n",
    "2. Stephen H. Friedberg, Arnold J. Insel, and Lawrence E. Spence, Linear Algebra.\n",
    "\n",
    "---\n",
    "\n",
    "### **Last Updated**\n",
    "**January 11, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector space\n",
    "\n",
    "**Definition (Vector space).**  \n",
    "A (real) vector space is a set $V$ on which two operations, $\\oplus_V$ (*vector addition*) and $\\otimes_V$ (*scalar multiplication*), are defined. They are defined so that\n",
    "\n",
    "- For each pair of elements $x,y$ in the vector space $V$, there is a unique element $x\\oplus_V y$ in $V$.\n",
    "- For each real number $c$ and each element $x$ in $V$, there is a unique element $c \\otimes_V x$ in $V$.\n",
    "\n",
    "and the following conditions hold:\n",
    "\n",
    "1. **Commutativity:** For all $x,y$ in $V$, $x\\oplus_V y = y \\oplus_V x$.\n",
    "2. **Associativity:** For all $x,y,z$ in $V$, $(x\\oplus_V y) \\oplus_V z = x \\oplus_V (y\\oplus_V z)$.\n",
    "3. **Existence of additive identity:** There exists an element in $V$, commonly denoted by $0_V$, such that $x \\oplus_V 0_V= x$ for all $x$ in $V$.\n",
    "4. **Existence of additive inverse:** For each $x$ in $V$ there exists a $y$ in $V$ such that $x \\oplus_V y = 0_V$. (We call $y$ the additive inverse of $x$.)\n",
    "5. **Scalar identity:** For each $x$ in $V$, $1_F \\otimes_V x = x$, where $1_F$ is the multiplicative identity in the real numbers.\n",
    "6. **Compatibility of scalar multiplication and real multiplication:** For each pair of real numbers $a,b$ and each $x$ in $V$, $(a\\cdot_F b) \\otimes_V x = a \\otimes_V (b \\otimes_V x)$.\n",
    "7. **Distributivity of scalar multiplication over vector addition:** For each real number $a$ and each pair of elements $x,y$ in $V$, $a \\otimes_V (x\\oplus_V y) = (a \\otimes_V x) \\oplus_V (a\\otimes_V y)$.\n",
    "8. **Distributivity of $\\otimes_V$ over $+_F$:** For each pair of real numbers $a,b$ and each element $x$ in $V$, $(a+_F b) \\otimes_V x = (a \\otimes_V x) \\oplus_V (b\\otimes_V y)$.\n",
    "\n",
    "---\n",
    "\n",
    "**Remark.**  \n",
    "This definition seems intimidating at first because of its length, but it's in fact a good thing that it's long! This means that the object in consideration is significantly more specific and less general than, say, whatever object that only satisfies the first few of these requirements.\n",
    "\n",
    "The point is that vector spaces are general enough that they encompass a wide variety of examples and mathematical phenomena, yet specific enough so that the theory associated to them is not overly abstract.\n",
    "\n",
    "---\n",
    "\n",
    "**Remark.**  \n",
    "We may also consider complex vector spaces, where the scalars are taken from the complex numbers. In general, one can also consider vector spaces over something a bit more general called a *field*, which leads us to the notion of *a vector space $V$ over a field $\\mathbb{F}$*. This widens the scope of theory considerably but it has many important applications.\n",
    "\n",
    "The most important class of vector spaces for us is the set of $n$-tuples, $\\mathbb{R}^n$.\n",
    "\n",
    "---\n",
    "\n",
    "**Example (Vectors with $n$-components form a vector space).**  \n",
    "Consider $V = \\mathbb{R}^n$. For any two vectors $(\\boldsymbol{v})  = v_i, (\\boldsymbol{w}) = w_i \\in \\mathbb{R}^n$, we define the operation $\\oplus_V$ so that $\\boldsymbol{v} \\oplus_V \\boldsymbol{w}$ is a vector in $\\mathbb{R}^n$ satisfying  \n",
    "$$\n",
    "(\\boldsymbol{v}  \\oplus_V \\boldsymbol{w} )_i = v_i + w_i.\n",
    "$$\n",
    "\n",
    "For any real number $c$, we also define scalar multiplication $\\otimes_V$ so that $c \\otimes_V \\boldsymbol{v}$ is a vector in $\\mathbb{R}^n$ satisfying  \n",
    "$$\n",
    "(c \\otimes_V \\boldsymbol{v})_i = c v_i.\n",
    "$$\n",
    "\n",
    "One can easily check that $(V, \\oplus_V, \\otimes_V)$ is a real vector space.\n",
    "\n",
    "However, there are many other examples as well. In the following example, we demonstrate why we need to distinguish between the addition and multiplication operators over the vector space $V$ and the underlying scalar field $\\mathbb{R}$.\n",
    "\n",
    "---\n",
    "\n",
    "**Example (Positive real numbers form a vector space under non-standard operations).**  \n",
    "Consider the set of positive numbers  \n",
    "$$\n",
    "V =  \\mathbb{R}^+ = \\{ x \\in \\mathbb{R} \\mid x > 0 \\}.\n",
    "$$\n",
    "\n",
    "Note that $V$ is *not* a vector space under the standard operations of vector addition and scalar multiplication over $\\mathbb{R}$. However, we may define the operator $\\oplus_V$ over $\\mathbb{R}^+$ via  \n",
    "$$\n",
    "x \\oplus_V y = xy,\n",
    "$$\n",
    "the standard scalar product between $x$ and $y$. We may also define $\\otimes_V$ over $\\mathbb{R}^+$ via  \n",
    "$$\n",
    "k \\otimes_V x = x^k = e^{k \\log x} = \\sum_{j=0}^\\infty \\frac{(k \\log x)^j}{j!},\n",
    "$$\n",
    "the standard scalar power of $x$.\n",
    "\n",
    "One can check that $(V, \\oplus_V, \\otimes_V)$ is a real vector space.\n",
    "\n",
    "---\n",
    "\n",
    "**Remark.**  \n",
    "In most of the examples that we will consider in this class, the operations $\\oplus_V, \\otimes_V$ will be inherited in one way or another from the real numbers. As such, we will use an abuse of notation and denote both the vector and scalar addition operators, as well as the vector and scalar multiplication operators, with `$+$` and `$\\cdot$`, and we denote a vector space $(V,+,\\cdot)$ as simply $V$.\n",
    "\n",
    "---\n",
    "\n",
    "**Example (The set of real $m \\times n$ matrices).**  \n",
    "Consider the collection of (real) $m \\times n$ matrices, which we'll denote by $\\mathcal{M}_{m \\times n}(\\mathbb{R})$. We define the sum of two matrices $A = (a_{ij}), B = (b_{ij})$ to be a matrix $A + B$ where the entry in the $i$-th row and the $j$-th column is  \n",
    "$$\n",
    "(A+B)_{ij} := a_{ij} + b_{ij}.\n",
    "$$\n",
    "\n",
    "We also define the scalar multiple of a matrix $A$ to be the matrix $cA$ where the entry in the $i$-th row and the $j$-th column is  \n",
    "$$\n",
    "(cA)_{ij} := c \\times a_{ij}.\n",
    "$$\n",
    "\n",
    "This is also a vector space over the real numbers.\n",
    "\n",
    "---\n",
    "\n",
    "**Example (Polynomials).**  \n",
    "Consider the collection of (real) polynomials of degree at most $n$ over the variable $x$, which we'll denote by $\\mathbb{P}_n[x]$. We define the sum of two polynomials $p(x) = \\sum_{j=0}^n p_j x^j, \\; q(x)= \\sum_{j=0}^n q_j x^j$ to be a polynomial $p+q$ satisfying  \n",
    "$$\n",
    "(p + q)(x) := (p_n + q_n) x^n + \\ldots + (p_1 + q_1) x + (p_0 + q_0),\n",
    "$$\n",
    "and the scalar multiple of a polynomial $p$ as the polynomial $cp$ satisfying\n",
    "$$\n",
    "(cp)(x) = (c p_n) x^n + \\ldots + (c p_0).\n",
    "$$\n",
    "\n",
    "This is a vector space over the real numbers.\n",
    "\n",
    "---\n",
    "\n",
    "**Example (Continuous functions).**  \n",
    "Consider the collection of continuous functions on the real line, which we'll denote by $C(\\mathbb{R})$. We define the sum of two functions $f,g$ to be the function $f+g$ satisfying\n",
    "$$\n",
    "(f + g) (x) = f(x) + g(x), \\; x \\in \\mathbb{R}\n",
    "$$\n",
    "and the scalar multiple of a function to be the function $cf$ satisfying\n",
    "$$\n",
    "(cf) (x) = c f(x), \\; x \\in \\mathbb{R}.\n",
    "$$\n",
    "This is an example of an *infinite dimensional vector space*. In this class our focus is on finite-dimensional vector spaces.\n",
    "\n",
    "---\n",
    "\n",
    "**Remark.**  \n",
    "When we are talking about vectors in a vector space, we're simply referring to elements of that vector space. So for example, if we were talking about vectors in the vector space of $m \\times n$ matrices, or the vector space of continuous functions, these vectors are *not* the usual elements of $\\mathbb{R}^n$. We'll try to be clear about what we mean, but one should keep in mind that from now on, what we call a vector can be different depending on context.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector subspaces\n",
    "\n",
    "Most sets we study in this class are subsets of a vector space.\n",
    "\n",
    "For example, if $A \\in \\mathcal{M}_{m \\times n}(\\mathbb{R})$, then\n",
    "\n",
    "1. The column space $\\text{Col}(A)$ is a subset of $\\mathbb{R}^m$.\n",
    "2. The nullspace $\\text{Null}(A)$ is a subset of $\\mathbb{R}^n$.\n",
    "\n",
    "We saw that the column space and the nullspace also have a vector space structure associated to them. This leads us to the notion of *vector subspaces*.\n",
    "\n",
    "**Definition (Vector subspaces).**  \n",
    "A subset $W$ of a vector space $V$ is said to be a *vector subspace* of $V$ if we can realize $W$ as a vector space (over the same field) with the operations of vector addition and scalar multiplication inherited from $V$.\n",
    "\n",
    "In other words, if $(V,+,\\cdot)$ is a vector space and $W$ is a subset of $V$, then $W$ is a subspace if $(W,+,\\cdot)$ is a vector space.\n",
    "\n",
    "When can we recognize that a subset of a vector space is a vector subspace? Notice that since a subset of a vector space is inheriting the same operations from the vector space itself, most of the requirements on the operations are already satisfied. It turns out that we only need to check the following.\n",
    "\n",
    "**Proposition (Criteria for subspace).**  \n",
    "A subset $W$ of a vector space $V$ is a vector subspace if and only if the following hold:\n",
    "\n",
    "1. $W$ contains the zero element $0_V \\in V$.\n",
    "2. $W$ is closed under addition: if $x,y$ are in $W$, then so is $x + y$.\n",
    "3. $W$ is closed under scalar multiplication: if $x$ is in $W$, then so is $c \\cdot x$ for any real number $c$.\n",
    "\n",
    "**Proof.**  \n",
    "*Exercise.*\n",
    "\n",
    "---\n",
    "\n",
    "**Remark.**  \n",
    "One may combine the second and third criteria and require $x + cy$ to belong to $W$ for any $x,y \\in W, c \\in \\mathbb{R}$.\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "The span of vectors in $\\mathbb{R}^n$ forms a vector subspace in $\\mathbb{R}^n$.\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Let $V$ be a vector space. The $\\text{Span}$ of vectors in $V$ forms a vector subspace in $V$.\n",
    "\n",
    "---\n",
    "\n",
    "**Prop. (Column space and nullspaces are subspaces).**  \n",
    "Let $A$ be an $m \\times n$ matrix. Then its column space $\\text{Col}(A)$ is a subspace of $\\mathbb{R}^m$ and its nullspace $\\text{Null}(A)$ is a subspace of $\\mathbb{R}^n$.\n",
    "\n",
    "**Proof.**  \n",
    "*Exercise.*\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "An immediate application of the previous proposition is that we can easily show certain sets are subspaces without checking the three criteria outlined above. For example, the set  \n",
    "$$\n",
    "\\left\\{ \\boldsymbol{v} \\in \\mathbb{R}^3 \\mid \\boldsymbol{v} = \\begin{pmatrix}\n",
    "x  \\\\\n",
    "y \\\\\n",
    "z \n",
    "\\end{pmatrix},  x + y + z = 0\\right\\}\n",
    "$$\n",
    "is a subspace because we can think of it as the nullspace of the matrix\n",
    "$$\n",
    "\\begin{pmatrix} \n",
    "1 & 1 & 1  \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Consider the set of vectors $\\boldsymbol{v}$ in $\\mathbb{R}^3$ satisfying  \n",
    "$$\n",
    "v_1 v_2 - v_3 = 0.\n",
    "$$\n",
    "This set is *not* a subspace, because\n",
    "$$\n",
    "\\begin{pmatrix} \n",
    "1 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "lies in this set, but\n",
    "$$\n",
    "\\begin{pmatrix} \n",
    "2 \\\\\n",
    "2\\\\\n",
    "2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "does not, since $2^2 - 2 = 2 \\neq 0$.\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Consider the set of vectors $\\boldsymbol{v}$ in $\\mathbb{R}^3$ such that  \n",
    "$$\n",
    "\\boldsymbol{v} = \\begin{pmatrix} \n",
    "v_1 \\\\ v_2 \\\\ v_3 \n",
    "\\end{pmatrix}  = \\begin{pmatrix} \n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{pmatrix} + c_1 \\begin{pmatrix} \n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "+ c_2 \\begin{pmatrix} \n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{pmatrix} \n",
    "$$\n",
    "for arbitrary scalars $c_1$ and $c_2$. This is *not* a subspace since the zero vector is not in this set.\n",
    "\n",
    "Sometimes we'll also need the following fact.\n",
    "\n",
    "**Proposition (Intersection of subspaces is a subspace).**  \n",
    "Let $U,W$ be subspaces of a vector space $V$. Then their intersection $U \\cap W$ is also a subspace of $V$.\n",
    "\n",
    "**Proof.**  \n",
    "Clearly $U \\cap V$ is a subset of $V$. Notice that:\n",
    "\n",
    "1. Since both $U,V$ contain the zero element $0_V$, their intersection $U \\cap V$ will also contain the zero element.\n",
    "2. Let $\\boldsymbol{x}_1, \\boldsymbol{x}_2$ be vectors in the intersection $U \\cap V$. We want to show that their sum is also in the intersection. Notice that if $\\boldsymbol{x}_1$ and $\\boldsymbol{x}_2$ are both in the intersection, then they are both in $U$ and $V$. Since $U$ and $V$ are both subspaces, both subspaces will contain the sum $\\boldsymbol{x}_1 + \\boldsymbol{x}_2$. So their sum lies in the intersection $U \\cap V$.\n",
    "3. Let $\\boldsymbol{x}$ be in $U \\cap V$. Then $\\boldsymbol{x}$ lies in both $U$ and $V$. Since they are both subspaces, any scalar multiple of $\\boldsymbol{x}$ also lies in both $U$ and $V$. So any scalar multiple of $\\boldsymbol{x}$ also lies in the intersection $U \\cap V$.\n",
    "\n",
    "However, the union of subspaces is not always a subspace.\n",
    "\n",
    "**Example.**  \n",
    "Consider $U = \\text{Span} \\,\\boldsymbol{e}_1$ and $V = \\text{Span} \\,\\boldsymbol{e}_2$. Then $U \\cup V$ contains both $\\boldsymbol{e}_1$ and $\\boldsymbol{e}_2$, but it doesn't contain $\\boldsymbol{e}_1 + \\boldsymbol{e}_2$.\n",
    "\n",
    "Subspaces are important because of the following fact: we can always characterize a vector space as the span of a “minimal set” of vectors. Here *minimal* refers to the fact that there are no redundancies when we consider linear combinations (for example in $\\mathbb{R}^3$, $\\text{Span}\\{\\boldsymbol{e}_1, \\boldsymbol{e}_2, \\boldsymbol{e}_3, \\boldsymbol{e}_1 + \\boldsymbol{e}_2 + \\boldsymbol{e}_3\\} = \\text{Span}\\{\\boldsymbol{e}_1, \\boldsymbol{e}_2, \\boldsymbol{e}_3\\}$).\n",
    "\n",
    "In other words, if a set is a vector space we can always identify a set of vectors that serve as the basic building blocks for these spaces. Any vector in the vector space then can be built from the basic building blocks (as a linear combination of them).\n",
    "\n",
    "This is the reason why highlighting the structure of the column space and the nullspace of a matrix is so important: since they are subspaces, there is hope to characterize them explicitly in terms of these basic building blocks.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear independence\n",
    "\n",
    "Next we'll discuss the notion of *independence* in a vector space. Before we move on we repeat the following remark.\n",
    "\n",
    "**Remark.**  \n",
    "When we are talking about vectors in a vector space, we're simply referring to elements of that vector space. So for example, if we are talking about vectors in the vector space of $m \\times n$ matrices, or the vector space of continuous functions, these vectors are not the usual elements of $\\mathbb{R}^n$.\n",
    "\n",
    "To illustrate this concept let us first consider the following examples.\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Consider the vectors\n",
    "$$\n",
    "\\boldsymbol{v}_1 =\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\boldsymbol{v}_2 =\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\boldsymbol{v}_3 =\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "What’s the span of $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\boldsymbol{v}_3$? Notice that the third vector is a linear combination of the first two vectors:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "This means that we don’t need the third vector when considering their span. Therefore,\n",
    "$$\n",
    "\\text{Span}\\{ \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\boldsymbol{v}_3 \\}\n",
    "=\n",
    "\\text{Span}\\{ \\boldsymbol{v}_1, \\boldsymbol{v}_2 \\}\n",
    "=\n",
    "\\mathbb{R}^2.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Consider the vectors\n",
    "$$\n",
    "\\boldsymbol{v}_1 =\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\boldsymbol{v}_2 =\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\boldsymbol{v}_3 =\n",
    "\\begin{pmatrix}\n",
    "2 \\\\\n",
    "1 \\\\\n",
    "3\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "What is the span of $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\boldsymbol{v}_3$? Notice that\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "2 \\\\\n",
    "1 \\\\\n",
    "3\n",
    "\\end{pmatrix}\n",
    "=\n",
    "2\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "1\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "so $\\boldsymbol{v}_3$ is a linear combination of $\\boldsymbol{v}_1$ and $\\boldsymbol{v}_2$. Once again, we can toss out $\\boldsymbol{v}_3$ when considering their $\\text{Span}$. Therefore\n",
    "$$\n",
    "\\text{Span}\\{ \\boldsymbol{v}_1, \\boldsymbol{v}_2, \\boldsymbol{v}_3 \\}\n",
    "=\n",
    "\\text{Span}\\{ \\boldsymbol{v}_1, \\boldsymbol{v}_2 \\}.\n",
    "$$\n",
    "\n",
    "Here, the $\\text{Span}$ is a plane, which is a two-dimensional subspace of $\\mathbb{R}^3$.\n",
    "\n",
    "What we’re seeing here is that, if we’re considering the $\\text{Span}$ of some vectors and one of them happens to be dependent on the other vectors in some way, then we can effectively throw away that vector when considering their $\\text{Span}$. In some sense, the appearance of that vector is *redundant* because it is already a linear combination of the other vectors in the set. This is the notion that we want to capture. Let’s record this as a definition.\n",
    "\n",
    "**Definition.**  \n",
    "A set of vectors is said to be *linearly independent* if none of the vectors can be written as the linear combination of the vectors in the set.\n",
    "\n",
    "This definition, while conceptually clear, is hard to apply in practice. Instead we will often use the following alternative definition.\n",
    "\n",
    "**Definition.**  \n",
    "A set of vectors $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\ldots, \\boldsymbol{v}_n$ is said to be *linearly independent* if we can establish the following implication:\n",
    "\n",
    "$$\n",
    "\\text{if } c_1 \\boldsymbol{v}_1 + \\ldots + c_n \\boldsymbol{v}_n = \\boldsymbol{0},\n",
    "\\text{ then } c_1 = c_2 = \\ldots = c_n = 0.\n",
    "$$\n",
    "\n",
    "Let’s try to see why the two definitions are equivalent to each other. Suppose there exists some non-zero $c_i$’s such that\n",
    "\n",
    "$$\n",
    "c_1 \\boldsymbol{v}_1 + \\ldots + c_k \\boldsymbol{v}_k = \\boldsymbol{0},\n",
    "$$\n",
    "\n",
    "perhaps up to relabeling of the indices. Then we can immediately write\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v}_1\n",
    "=\n",
    "-\\frac{1}{c_1}\n",
    "\\bigl(\n",
    "c_2 \\boldsymbol{v}_2 + \\ldots + c_k \\boldsymbol{v}_k\n",
    "\\bigr),\n",
    "$$\n",
    "\n",
    "meaning that $\\boldsymbol{v}_1$ is a linear combination of other vectors in the set. If a vector is already a linear combination of the other vectors, then we can find non-zero coefficients to produce the zero vector. So the two definitions are equivalent to each other.\n",
    "\n",
    "**Remark.**  \n",
    "According to this definition, any set containing the $\\boldsymbol{0}$ vector is automatically linearly dependent.\n",
    "\n",
    "Let’s see how we can check linear independence in practice.\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Consider the vectors\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v}_1 =\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\boldsymbol{v}_2 =\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "It should be clear that they’re linearly independent, but let’s check using the alternative definition. If\n",
    "\n",
    "$$\n",
    "c_1 \\boldsymbol{v}_1 + c_2 \\boldsymbol{v}_2 = \\boldsymbol{0},\n",
    "$$\n",
    "\n",
    "then we must have\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "c_1 \\\\\n",
    "c_2\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "meaning that $c_1 = c_2 = 0$. So they are linearly independent.\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Consider the vectors\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v}_1 =\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "3 \\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\boldsymbol{v}_2 =\n",
    "\\begin{pmatrix}\n",
    "3 \\\\\n",
    "-1 \\\\\n",
    "4\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\boldsymbol{v}_3 =\n",
    "\\begin{pmatrix}\n",
    "2 \\\\\n",
    "1 \\\\\n",
    "2\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Now it’s not immediately clear whether they are linearly independent. Let’s check with the alternative definition. If\n",
    "\n",
    "$$\n",
    "c_1 \\boldsymbol{v}_1 + c_2 \\boldsymbol{v}_2 + c_3 \\boldsymbol{v}_3 = \\boldsymbol{0},\n",
    "$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol{v}_1\n",
    "&  \\\\\n",
    "\\boldsymbol{v}_2\n",
    "&  \\\\\n",
    "\\boldsymbol{v}_3\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "c_1 \\\\\n",
    "c_2 \\\\\n",
    "c_3\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\boldsymbol{0}.\n",
    "$$\n",
    "\n",
    "This means that we can view the coefficients as an element of the nullspace. Recall that the nullspace of a matrix $A$ is equal to the nullspace of its row reduced echelon form $\\mathrm{rref}(A)$, so we can see if we can find non-trivial solutions by performing elimination on the matrix formed by $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\boldsymbol{v}_3$. This is the matrix\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1 & 3 & 2 \\\\\n",
    "3 & -1 & 1 \\\\\n",
    "0 & 4 & 2\n",
    "\\end{pmatrix}\n",
    "\\sim\n",
    "\\begin{pmatrix}\n",
    "1 & 3 & 2 \\\\\n",
    "0 & -10 & -5 \\\\\n",
    "0 & 4 & 2\n",
    "\\end{pmatrix}\n",
    "\\sim\n",
    "\\begin{pmatrix}\n",
    "1 & 3 & 2 \\\\\n",
    "0 & 2 & 1 \\\\\n",
    "0 & 4 & 2\n",
    "\\end{pmatrix}\n",
    "\\sim\n",
    "\\begin{pmatrix}\n",
    "1 & 3 & 2 \\\\\n",
    "0 & 2 & 1 \\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "We can stop here: the matrix only has two pivots, which means that there is one free variable. This shows us that there are non-zero solutions, so $\\boldsymbol{v}_1, \\boldsymbol{v}_2, \\boldsymbol{v}_3$ are linearly dependent.\n",
    "\n",
    "If you want, you can choose $c_1 = c_2 = 1$ and $c_3 = -2$.\n",
    "\n",
    "From this example we see that for vectors in $\\mathbb{R}^n$, we can provide the following equivalent characterization of linear independence.\n",
    "\n",
    "**Proposition.**  \n",
    "A set of vectors $\\boldsymbol{v}_1, \\ldots, \\boldsymbol{v}_n \\in \\mathbb{R}^m$ are linearly independent if $A \\boldsymbol{x} = \\boldsymbol{0}_m$ only has the zero solution $\\boldsymbol{0}_n$, where $A$ is the matrix\n",
    "$$\n",
    "A =\n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol{v}_1\n",
    "& \n",
    "\\ldots\n",
    "& \n",
    "\\boldsymbol{v}_n\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "**Proof.**  \n",
    "This is just writing the alternative definition in matrix notation.\n",
    "\n",
    "However, note carefully that this characterization only applies to vectors in $\\mathbb{R}^n$. The earlier definition is more general because we can use it to define the linear independence of elements in a general vector space.\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Let $V = \\mathbb{P}_2[x]$ and consider the set $S = \\{x , x^2\\}$. To check that this set is linearly independent, we need to check that if\n",
    "$$\n",
    "c_1 x + c_2 x^2 = 0_V,\n",
    "$$\n",
    "then we must have $c_1 = c_2 = 0$. Note that $0_V$ here is the zero polynomial, which evaluates to the number 0 for any $x \\in \\mathbb{R}$. Therefore we must have\n",
    "$$\n",
    "c_1 + c_2 = 0\n",
    "\\quad \\text{and} \\quad\n",
    "2c_1 + 4c_2 = 0\n",
    "\\implies\n",
    "c_1 = c_2 = 0.\n",
    "$$\n",
    "\n",
    "One last thing to note before we move on to the next section: this gives another criterion for the invertibility of a square matrix.\n",
    "\n",
    "**Corollary.**  \n",
    "A square matrix $A$ is invertible if and only if the columns of $A$ are linearly independent.\n",
    "\n",
    "**Proof.**  \n",
    "This follows from the fact that $A$ is invertible if and only if $A \\boldsymbol{x} = \\boldsymbol{0}$ only admits the trivial solution $\\boldsymbol{x} = \\boldsymbol{0}$ if and only if $\\text{Null}(A) = \\{\\boldsymbol{0}\\}$.\n",
    "\n",
    "We can add this to the list of equivalent criteria for determining the invertibility of a matrix.\n",
    "\n",
    "---\n",
    "\n",
    "## Basis\n",
    "\n",
    "Once we have the notion of linear independence, we can introduce the notion of a *basis*.\n",
    "\n",
    "**Definition.**  \n",
    "Let $V$ be a vector space and let $\\mathcal{B}$ be a collection of vectors in $V$. If the vectors in $\\mathcal{B}$ span $V$ (this means that $V = \\text{Span} \\,\\mathcal{B}$) and the vectors are linearly independent, then we call $\\mathcal{B}$ a *basis* for $V$.\n",
    "\n",
    "Therefore there are two criteria for a set $\\mathcal{B}$ to be a basis for a vector space $V$:\n",
    "\n",
    "1. We need $V = \\text{Span}\\,\\mathcal{B}$; in other words, every vector in $V$ must be a linear combination of the vectors in $\\mathcal{B}$.\n",
    "2. We also need $\\mathcal{B}$ to be linearly independent.\n",
    "\n",
    "Let’s consider some examples.\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Consider\n",
    "$$\n",
    "\\mathcal{B} =\n",
    "\\biggl\\{\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\\biggr\\}.\n",
    "$$\n",
    "\n",
    "$\\mathcal{B}$ is a basis for $\\mathbb{R}^2$, because the $\\text{Span}$ of these two vectors is $\\mathbb{R}^2$ and they are also linearly independent. This basis is referred to as the *standard basis* for $\\mathbb{R}^2$.\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Consider\n",
    "$$\n",
    "\\mathcal{B} =\n",
    "\\biggl\\{\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "1\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\\biggr\\}.\n",
    "$$\n",
    "\n",
    "One can check that $\\mathcal{B}$ is also a basis for $\\mathbb{R}^2$, because the $\\text{Span}$ of these two vectors is $\\mathbb{R}^2$ and they are also linearly independent. This shows that the basis for a vector space need not be unique. Though, notice that both bases have the same number of vectors in them.\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Consider\n",
    "$$\n",
    "\\mathcal{B} =\n",
    "\\biggl\\{\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "\\biggr\\}.\n",
    "$$\n",
    "\n",
    "Is $\\mathcal{B}$ a basis for $\\mathbb{R}^3$? No, because the $\\text{Span}$ of these two vectors is not $\\mathbb{R}^3$ but rather a two-dimensional subspace of $\\mathbb{R}^3$.\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Consider\n",
    "$$\n",
    "\\mathcal{B} =\n",
    "\\biggl\\{\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{pmatrix}\n",
    "\\biggr\\}.\n",
    "$$\n",
    "\n",
    "Is $\\mathcal{B}$ a basis for $\\mathbb{R}^3$? The $\\text{Span}$ of the four vectors is $\\mathbb{R}^3$, however the fourth vector is a linear combination of the first three. So $\\mathcal{B}$ is *not* a basis for $\\mathbb{R}^3$.\n",
    "\n",
    "In some sense, a basis is a set containing the “right” amount of vectors that span the space. In fact, the following remarkable results are true.\n",
    "\n",
    "**Theorem.**  \n",
    "Every vector space admits a (Hamel) basis.\n",
    "\n",
    "**Proof.**  \n",
    "The standard proof uses Zorn’s lemma, which is outside the scope of this course. The interesting fact about this result is that this is equivalent to the axiom of choice, which is equivalent to the statement that the Cartesian product of a collection of non-empty sets is also non-empty.\n",
    "\n",
    "---\n",
    "\n",
    "**Remark.**  \n",
    "A Hamel basis $\\mathcal{H}$ for a vector space $V$ is a linearly independent set for which every element in $V$ can be written as a *finite* linear combination of elements in $\\mathcal{H}$. In the finite-dimensional setting, this is equivalent to the standard definition of a basis. However, in the infinite-dimensional setting Hamel bases are sometimes not very useful. In some areas of mathematics, the notion of a *Schauder basis* is more appropriate, as it allows one to represent elements as *infinite* linear combinations of elements in a Schauder basis. In that setting, the question of convergence arises, and one typically needs to impose more than just a vector space structure on a set to make sense of such sums. Since we focus on the analysis of finite-dimensional vector spaces in this course, we will not need to worry about these nuances.\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem.**  \n",
    "Let $V$ be a vector space and let $J = \\{\\boldsymbol{w}_1, \\ldots , \\boldsymbol{w}_m\\}$ be such that $\\text{Span}\\,J = V$ for $m \\in \\mathbb{N}$. If $I = \\{\\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_n \\}$ is a linearly independent set in $V$ for $n \\in \\mathbb{N}$, then $n \\le m$.\n",
    "\n",
    "**Proof.**  \n",
    "It suffices to prove the contrapositive: if $n > m$, then $I$ is linearly dependent. Note that since $V = \\text{Span}\\,J$, every element in $I$ can be written as a linear combination of elements in $J$. Therefore we can find scalars $a_{ij}, 1 \\le i \\le m, 1 \\le j \\le n$ such that\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v}_j\n",
    "=\n",
    "\\sum_{i=1}^m a_{ij} \\boldsymbol{w}_i\n",
    "$$\n",
    "\n",
    "for every $1 \\le j \\le n$. Note that for $c_1, \\ldots , c_n \\in \\mathbb{R}$,\n",
    "\n",
    "$$\n",
    "c_1 \\boldsymbol{v}_1 + \\ldots  + c_n \\boldsymbol{v}_n\n",
    "=\n",
    "\\sum_{j=1}^n c_j \\boldsymbol{v}_j\n",
    "=\n",
    "\\sum_{j=1}^n c_j\n",
    "\\Bigl(\n",
    "\\sum_{i=1}^m a_{ij} \\boldsymbol{w}_i\n",
    "\\Bigr)\n",
    "=\n",
    "\\sum_{i=1}^m\n",
    "\\Bigl(\\sum_{j=1}^n  c_j a_{ij} \\Bigr)\n",
    "\\boldsymbol{w}_i.\n",
    "$$\n",
    "\n",
    "Note that if $n > m$, then there exists $c_1, \\ldots , c_n$ not all zero for which\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^n c_j a_{ij}  = 0\n",
    "\\quad \\text{for every} \\quad 1 \\le i \\le m.\n",
    "$$\n",
    "\n",
    "Therefore there exists $c_1, \\ldots, c_n$ not all zero for which $c_1 \\boldsymbol{v}_1 + \\ldots  + c_n \\boldsymbol{v}_n = \\boldsymbol{0}$, showing that $I$ is linearly dependent.\n",
    "\n",
    "---\n",
    "\n",
    "**Proposition.**  \n",
    "Let $V$ be a vector space and let $J = \\{\\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_m \\}$ be a basis for $V$, for $m \\in \\mathbb{N}$. Then any other basis for $V$ has the same number of elements in it.\n",
    "\n",
    "**Proof.**  \n",
    "By the theorem above, every basis for $V$ must be finite and contains no more than $m$ elements. Thus if $I = \\{ \\boldsymbol{w}_1, \\ldots , \\boldsymbol{w}_n\\}$ is another basis for $V$, we must have $n \\le m$. However, by the same argument, we must also have $m \\le n$ as $I$ spans $V$ and $J$ is linearly independent. Therefore we must have $n = m$.\n",
    "\n",
    "---\n",
    "\n",
    "## Dimension\n",
    "\n",
    "Since all bases of a vector space have the same number of vectors in them, we can use this number to define the *dimension* of the space.\n",
    "\n",
    "**Definition.**  \n",
    "Let $V$ be a vector space. If $\\mathcal{B} = \\{\\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_n \\}$ is a basis for $V$ and $n \\in \\mathbb{N}$, then the *dimension* of $V$ is $n$. Since $\\mathcal{B}$ is a set with finitely many elements, we refer to $V$ in this case as a *finite-dimensional* vector space. If $V$ does not admit a basis with finitely many elements, then we say that $V$ is an *infinite-dimensional* vector space.\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Consider the standard basis for $\\mathbb{R}^3$:\n",
    "\n",
    "$$\n",
    "\\mathcal{B} =\n",
    "\\Bigl\\{\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "\\Bigr\\}.\n",
    "$$\n",
    "\n",
    "$\\mathcal{B}$ contains 3 vectors, so the dimension of $\\mathbb{R}^3$ is three.\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Consider the set\n",
    "\n",
    "$$\n",
    "B =\n",
    "\\Bigl\\{\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "\\Bigr\\}.\n",
    "$$\n",
    "\n",
    "While it’s not a basis for $\\mathbb{R}^3$, it *is* a basis for the vector subspace\n",
    "\n",
    "$$\n",
    "\\text{Span}\n",
    "\\Bigl\\{\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0\n",
    "\\end{pmatrix}\n",
    "\\Bigr\\}.\n",
    "$$\n",
    "\n",
    "Since $B$ has two elements, the dimension of this vector subspace is 2.\n",
    "\n",
    "We see in all of these examples that this notion of dimension coincides with our intuitive notion of dimension coming from geometry.\n",
    "\n",
    "---\n",
    "\n",
    "## The rank of a matrix\n",
    "\n",
    "Next we discuss the notion of a *rank* of a matrix. The rank of a matrix tells us about the dimension of the column space $\\text{Col}(A)$ and nullspace $\\text{Null}(A)$.\n",
    "\n",
    "First, we need to define two separate ranks, the row rank and the column rank.\n",
    "\n",
    "**Definition.**  \n",
    "The *row rank* of a matrix $A$ is the number of independent rows in $A$.\n",
    "\n",
    "**Definition.**  \n",
    "The *column rank* of a matrix $A$ is the number of independent columns in $A$.\n",
    "\n",
    "What we want to show at the end of this section is that the row rank of a matrix is equal to its column rank. To do this we need the following ingredients.\n",
    "\n",
    "---\n",
    "\n",
    "**Proposition.**  \n",
    "The row rank and column rank of a matrix $A$ do not change under elementary row operations.\n",
    "\n",
    "**Proof.**  \n",
    "The fact that the row rank doesn’t change is more obvious: when we’re performing elementary row operations, we are taking linear combinations of the rows. So the number of independent rows do not change.\n",
    "\n",
    "The fact that the column rank does not change is less obvious, though it is still true. Note that we can view elementary row operations as an elimination matrix $E$ acting on $A$. So if\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol{v}_1\n",
    "& \n",
    "\\ldots\n",
    "& \n",
    "\\boldsymbol{v}_n\n",
    "\\end{pmatrix},\n",
    "$$\n",
    "\n",
    "the matrix you obtain after elementary row operations is\n",
    "\n",
    "$$\n",
    "E A =\n",
    "\\begin{pmatrix}\n",
    "E \\boldsymbol{v}_1\n",
    "& \n",
    "\\ldots\n",
    "& \n",
    "E \\boldsymbol{v}_n\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "However, note that $E$ is an invertible matrix, and we can observe by definition that $\\boldsymbol{v}_{n_1}, \\ldots ,\\boldsymbol{v}_{n_k}$ are linearly independent if and only if $E\\boldsymbol{v}_{n_1}, \\ldots ,E\\boldsymbol{v}_{n_k}$ are linearly independent. This means that $E$ is preserving the number of independent columns, so the column rank does not change under elementary row operations.\n",
    "\n",
    "---\n",
    "\n",
    "**Remark.**  \n",
    "A closer look at this proof also shows that the matrix $E$ is preserving the position of the independent columns. In practice, this gives us a way to find a basis for the column space. First, we row-reduce a matrix $A$ to its row reduced echelon form $\\mathrm{rref}(A)$ (or any echelon form, since we are only interested in the pivot positions). Then, we look at its independent columns. What this observation shows us is that the independent columns of the *original* matrix $A$ are precisely the columns corresponding to the independent columns of $\\mathrm{rref}(A)$. So these columns of $A$ will form a basis for the column space $\\text{Col}(A)$. We’ll see some examples of this very soon.\n",
    "\n",
    "As an immediate corollary, we know that a matrix $A$ and its row reduced echelon form have the same row rank and column rank.\n",
    "\n",
    "Next, we show that the column rank is equal to the number of pivots in $\\mathrm{rref}(A)$.\n",
    "\n",
    "---\n",
    "\n",
    "**Proposition.**  \n",
    "Let $\\mathrm{rref}(A) \\in \\mathcal{M}_{m \\times n}(\\mathbb{R})$ be a matrix in row reduced echelon form. Then the columns containing pivots are linearly independent, and the non-pivot columns can be written as linear combinations of the pivot columns.\n",
    "\n",
    "**Proof.**  \n",
    "If a column contains a pivot, then the entries above and below the pivot are zero. By the definition of the row reduced echelon form, no two columns containing pivots will have the pivots in the same row. So they must be linearly independent from another because the standard basis vectors are linearly independent.\n",
    "\n",
    "As for the non-pivot columns, suppose $\\mathrm{rref}(A)$ admits $r$ pivots and for every $1 \\le i \\le m$ we define $j_i$ to be the column index of the pivot in row $i$. If the column index of a non-pivot column is $j$, then we must either have $j \\le j_1, j \\ge j_r$, or $j_i \\le j \\le j_{i+1}$ for some $1 \\le i \\le r-1$. In the first case, if $j \\le j_1$, then by the definition of $\\mathrm{rref}(A)$, the $j$-th column is a column of zeros, so it is trivially a linear combination of the pivot columns. If $j \\ge j_r$, then by the definition of $\\mathrm{rref}(A)$ again, the entries below the $r$-th row must all be zero, and therefore the non-zero entries in the $j$-th column can only be in rows $1$ through $r$. By definition of $\\mathrm{rref}(A)$, the pivot columns are standard basis vectors $\\boldsymbol{e}_1, \\ldots , \\boldsymbol{e}_{r}$, therefore the $j$-th column can be written as a linear combination of the pivot columns. Similarly, if $j_i \\le j \\le j_{i+1}$ for $1 \\le i \\le r-1$, then by the definition of $\\mathrm{rref}(A)$, the non-zero entries in the $j$-th column in rows $1$ through $i$, and therefore it can be written as a linear combination of the pivot columns $\\boldsymbol{e}_1, \\ldots , \\boldsymbol{e}_i , \\ldots , \\boldsymbol{e}_r$.\n",
    "\n",
    "This proposition shows that the number of independent columns in $\\mathrm{rref}(A)$ are precisely the columns containing the pivots, so the column rank of $\\mathrm{rref}(A)$ is equal to the number of pivots in $\\mathrm{rref}(A)$.\n",
    "\n",
    "It turns out that the number of free variables also gives us the dimension of the nullspace of $\\mathrm{rref}(A)$. Roughly speaking, this is because they tell us how many parameters there are in the solutions to $\\mathrm{rref}(A)\\boldsymbol{x} = 0$, and we can utilize these parameters to find a basis for $N(\\mathrm{rref}(A))$ that has the same number of elements as there are free variables. Since we’ve shown that $A$ and $\\mathrm{rref}(A)$ have the same nullspace, the number of free variables in $\\mathrm{rref}(A)$ will also tell us the dimension of the nullspace of $A$.\n",
    "\n",
    "We should also notice that for a matrix in row reduced echelon form, the number of pivots *plus* the number of free variables is equal to the number of columns in the matrix. So putting this all together, we have the rank-nullity theorem.\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem (Rank-nullity theorem).**  \n",
    "Let $A$ be an $m \\times n$ matrix. Then\n",
    "$$\n",
    "\\dim \\text{Col}(A) + \\dim \\text{Null}(A) = n.\n",
    "$$\n",
    "\n",
    "**Proof.**  \n",
    "First, note that the dimension of the column space $\\text{Col}(A)$ is given by the number of independent columns in $A$. By the proposition above, the dimension of the column space $\\text{Col}(A)$ is the same as that of $R(\\mathrm{rref}(A))$. We’ve also shown that the dimension of the column space $R(\\mathrm{rref}(A))$ is given by the number of pivots in $\\mathrm{rref}(A)$. Since the number of pivots plus the number of free variables is $n$, and the number of free variables is also the dimension of the nullspace $N(\\mathrm{rref}(A))$ (and also $\\text{Null}(A)$), we have\n",
    "\n",
    "$$\n",
    "\\dim R(\\mathrm{rref}(A)) + \\dim N(\\mathrm{rref}(A)) = n,\n",
    "$$\n",
    "\n",
    "and by the argument above this implies that\n",
    "\n",
    "$$\n",
    "\\dim \\text{Col}(A) + \\dim \\text{Null}(A) = n.\n",
    "$$\n",
    "\n",
    "**Remark.**  \n",
    "Note that the proof here is straightforward because we assumed that $\\dim \\text{Null}(A)$ is equal to the number of free variables. Proving this independently requires a bit of work, which is why we’re omitting it for now.\n",
    "\n",
    "Now we are ready to state the main theorem of this section.\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem (Row rank equals column rank).**  \n",
    "The row rank of a matrix is equal to its column rank.\n",
    "\n",
    "**Proof.**  \n",
    "Here we use a somewhat slick proof. For now, let’s refer to the column rank of a matrix $A$ simply as $\\text{rank} A$. Recall that the transpose of $A$ is the matrix whose rows are the columns of $A$. So if we can show that $\\text{rank} A = \\text{rank} A^T$, we’ve shown that the row rank is equal to the column rank.\n",
    "\n",
    "First, note that $A^T A \\boldsymbol{x} = \\boldsymbol{0}$ if and only if $A \\boldsymbol{x} = \\boldsymbol{0}$. One direction is obvious. For the other direction, note that\n",
    "\n",
    "$$\n",
    "A^T A \\boldsymbol{x} = \\boldsymbol{0}\n",
    "\\implies\n",
    "\\boldsymbol{x}^T A^T A \\boldsymbol{x} = \\boldsymbol{0}\n",
    "\\implies\n",
    "(A\\boldsymbol{x})^T A \\boldsymbol{x} = \\boldsymbol{0}\n",
    "\\implies\n",
    "(A \\boldsymbol{x}) \\cdot (A \\boldsymbol{x}) = \\boldsymbol{0}\n",
    "\\implies\n",
    "\\|A \\boldsymbol{x}\\|^2 = \\boldsymbol{0}\n",
    "\\implies\n",
    "A \\boldsymbol{x} = \\boldsymbol{0}.\n",
    "$$\n",
    "\n",
    "This shows that $A^TA$ and $A$ have the same nullspace (we will use this fact again very soon). Since $A^TA$ and $A$ have the same number of columns, by the rank-nullity theorem we know that the dimensions of their column spaces must be equal.\n",
    "\n",
    "Now notice that using the column interpretation of matrix multiplication, each column of $A^T A$ is a linear combination of the columns of $A^T$. So the column space of $A^T A$ is a subspace of the column space of $A^T$. Therefore, $\\text{rank} A^T A \\le \\text{rank} A^T$.\n",
    "\n",
    "Since we’ve shown that $A$ and $A^T A$ have the same column rank, this shows that $\\text{rank} A \\le \\text{rank} A^T$.\n",
    "\n",
    "However, notice that we can run the exact same argument through $A^T$ and $(A^T)^T = A$! So at the end, we can reach the conclusion that $\\text{rank} A^T \\le \\text{rank} A$.\n",
    "\n",
    "So at the end of the day, we must have that the column rank of $A$ is equal to the column rank of $A^T$.\n",
    "\n",
    "From now on, we’ll refer to both of these numbers as the *rank* of the matrix.\n",
    "\n",
    "---\n",
    "\n",
    "## Putting it together\n",
    "\n",
    "Let $A$ be an $m \\times n$ matrix. Now we can finally put these notions together and use them to understand the linear system $A \\boldsymbol{x}  = \\boldsymbol{b}$. One way to think of linear systems is to think of them as\n",
    "\n",
    "$$\n",
    "A \\boldsymbol{x} = \\boldsymbol{b} + \\boldsymbol{0}.\n",
    "$$\n",
    "\n",
    "To solve the full problem there are two subproblems that we want to solve.\n",
    "\n",
    "1. **Identify the column space $\\text{Col}(A)$.**  \n",
    "   This will allow us to characterize the solvability of the problem, and also help us find a particular solution to $A \\boldsymbol{x} = \\boldsymbol{b}$. If $\\boldsymbol{b}$ lies in the column space, then there’s hope for finding a particular solution. To do this we look at the row reduced echelon form of $A$. We’ve shown that the column space of $A$ and $\\mathrm{rref}(A)$ are generally different, but they have the same dimension, and they are both spanned by the corresponding independent columns. This observation can help us identify a basis for the column space. Once we identify a basis for the column space, we look to write $\\boldsymbol{b}$ as a linear combination of the elements in the basis. The coefficients will give us the particular solution $\\boldsymbol{x}_p$. This is because of the column picture:\n",
    "   $$\n",
    "   A \\boldsymbol{x}_p\n",
    "   =\n",
    "   c_1 \\boldsymbol{v}_1\n",
    "   +\\ldots\n",
    "   + c_n \\boldsymbol{v}_n\n",
    "   =\n",
    "   \\boldsymbol{b}.\n",
    "   $$\n",
    "   The coefficients corresponding to the dependent columns will be 0.\n",
    "\n",
    "2. **Solve $A \\boldsymbol{x} = \\boldsymbol{0}$.**  \n",
    "   This amounts to understanding the nullspace of $A$. The nullspace of $A$ is the same as the nullspace of its row reduced echelon form $\\mathrm{rref}(A)$, so we can just look at the nullspace of $\\mathrm{rref}(A)$. The nullspace is also a subspace, and it has the same dimension as the number of free variables in the system. This is also the number of dependent columns in $A$ by the rank-nullity theorem.\n",
    "\n",
    "---\n",
    "\n",
    "## Some more useful results\n",
    "\n",
    "Here are a few results that we often use either explicitly or implicitly in practice.\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem (The basis theorem).**  \n",
    "Let $V$ be a finite-dimensional vector space with $\\dim V = n$. Let $\\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_n$ be $n$ vectors in $V$. Then the following are equivalent.\n",
    "\n",
    "a) $\\text{Span}\\{\\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_n \\} = V$.  \n",
    "b) $\\boldsymbol{v}_1, \\ldots, \\boldsymbol{v}_n$ are linearly independent.\n",
    "\n",
    "In either case $\\mathcal{B} = \\{\\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_n \\}$ is a basis for $V$.\n",
    "\n",
    "**Proof.**  \n",
    "*Homework problem.*\n",
    "\n",
    "---\n",
    "\n",
    "**Remark.**  \n",
    "Note carefully that we need the same number of vectors as the dimension of the vector space in order for these two notions to be equivalent. In fact, the following is true.\n",
    "\n",
    "**Proposition.**  \n",
    "Let $V$ be a finite-dimensional vector space and $\\dim V = n$. Suppose $\\boldsymbol{v}_1, \\ldots, \\boldsymbol{v}_m$ are $m$ vectors in $V$. If $m > n$, then they must be linearly dependent; if $m < n$, then they cannot span $V$.\n",
    "\n",
    "**Proof.**  \n",
    "This follows directly from a previous proposition.\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem (Dimension and subspaces).**  \n",
    "Let $V$ be a finite-dimensional vector space, and let $W$ be a subspace of $V$. Then $\\dim W \\le \\dim V$, and $\\dim W = \\dim V$ if and only if $W = V$.\n",
    "\n",
    "**Proof.**  \n",
    "If $\\mathcal{B} = \\{\\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_n \\}$ is a basis for $W$, then $\\dim W = n$ and $\\mathcal{B}$ is linearly independent. Since $\\mathcal{B}$ is also a linearly independent subset of $V$, we immediately have $\\dim W = n \\le \\dim V$. The second part of the theorem follows from the basis theorem above, but we can also prove it independently. This will be done in recitation.\n",
    "\n",
    "---\n",
    "\n",
    "**Example.**  \n",
    "Suppose you have a $3 \\times 4$ matrix $A$ that has 3 pivots. Then $\\dim \\text{Col}(A) = 3$. Since $\\text{Col}(A)$ is a subspace of $\\mathbb{R}^3$ and $\\dim \\mathbb{R}^3 = 3$, by the theorem above we know that we must have $\\text{Col}(A) = \\mathbb{R}^3$.\n",
    "\n",
    "---\n",
    "\n",
    "**Proposition (Spanning sets in $\\mathbb{R}^n$ in terms of $A \\boldsymbol{x} = \\boldsymbol{b}$).**  \n",
    "Suppose $W$ is a subspace of $\\mathbb{R}^n$. Then a set of vectors $\\{\\boldsymbol{v}_1, \\ldots , \\boldsymbol{v}_k\\}$ spans $W$ if and only if the system $A \\boldsymbol{x} = \\boldsymbol{b}$ is consistent for all $\\boldsymbol{b} \\in W$ where\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol{v}_1\n",
    "& \n",
    "\\ldots\n",
    "& \n",
    "\\boldsymbol{v}_k\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "**Proof.**  \n",
    "This is just rewriting the definition of span.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
